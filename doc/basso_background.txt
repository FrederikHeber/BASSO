////
#  Project: BASSO - BAnach Sequential Subspace Optimizer
#  Description: C++ library for solving optimization problems in Banach spaces
#  Copyright (C)  2016-2019 Frederik Heber. All rights reserved.
#
#
#    This file is part of the BASSO library.
#
#     BASSO is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 2 of the License, or
#     (at your option) any later version.
#
#     BASSO is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with BASSO.  If not, see <http://www.gnu.org/licenses/>.
#
////

[[background]]
Background
----------

One of the most ubiquitous tasks in computing is solving linear systems of
equations,
latexmath:[$A x = y$]
where a matrix latexmath:[$A$] and a right-hand side vector latexmath:[$y$] are
given and we are looking for the solution vector latexmath:[$x$]. Because of the
omnipresence of this problem, many different solution methods exist, whose
applicability depends on the properties of the matrix: when the matrix is
symmetric, one might use Cholesky factorization. If the matrix is symmetric and
positive definite, and especially if it is very large, Conjugate Gradient
schemes are typically employed.

If the matrix is generic, e.g., it is not symmetric, then more general schemes
are necessary.

From a more mathematical perspective, latexmath:[$x$] is the element of a vector
space latexmath:[$\mathrm{X}$]. Moreover, the matrix is a linear operator
mapping between this space latexmath:[$\mathrm{X}$] and and itself or maybe
even another space latexmath:[$\mathrm{Y}$] that contains the given right-hand
side latexmath:[$y$].

We are looking at the more interesting cases, where there is either no solution
latexmath:[$x$] or infinitely many, i.e., the system is over- oder
under-determined with a non-unique solution. Then, in order to define a
"closest" solution in the sense latexmath:[$||Ax-y||$], we need norms that
induce a metric. Therefore, the vector spaces are normed vector spaces.

When the row and column dimensions are not the same, then we certainly do not
have a single space, but two spaces. This is also the case when different norms
apply to source and target space. Moreover, when the norm is not an
latexmath:[$\ell_2$]-norm, then latexmath:[$\mathrm{X}$] is not even a Hilbert
space.

With an under-determined or over-determined system of equation the linear
operator is in general not invertible. The remedy is called regularization where
the linear operator is modified in such a way as to become invertible but still
as faithful as possible to its original version. More generally, regularization
is required if the right-hand side is not in the range of the operator. This may
also occur for the case of "noisy data", i.e., when the right-hand side
latexmath:[$\widetilde{y} = y + \delta y$] is perturbed by white-noise
latexmath:[$\delta y$].

Typically, (Tikhonov-)regularization is formulated as a minimization problem of
a functional latexmath:[$F(x)$] by adding a penalty term as follows:

latexmath:[$min_x ( || Ax-y ||^2 + \lambda ||x||^2) =: min_x F(x)$].

This minimization problems contains two terms: the first is called the data
fidelity term that minimizes the residual, i.e. the metric distance to the sets
of admissible solutions. The second term is the regularization term that
enforces a minimum-norm solution.

Through the minimum norm requirement a solution can be sought that is equipped
with certain favorable properties. For example, a sparse solution is obtained
of the latexmath:[$\ell_1$]-norm is used.

As already mentioned, depending on the employed norms in this minimization
problem, the solutions no longer live in a Hilbert space but in a more general
Banach space, e.g., the N-dimensional vector space latexmath:[$\mathrm{R}^N$]
equipped with the norm latexmath:[\ell_{1.1}] norm.  Convergence theoretical
results are well-established for Hilbert spaces. For Banach spaces the situation
is more difficult: In a Banach space the space and its dual do not in general
coincide, namely they are not in general isometric isomorphic to each other.

This in turn causes us to deal not with two spaces, latexmath:[$\mathrm{X}$]
and latexmath:[$\mathrm{Y}$], but with possibly up to four spaces,
latexmath:[$\mathrm{X}$], latexmath:[$\mathrm{X}^\ast$], latexmath:[$\mathrm{Y}$],
and latexmath:[$\mathrm{Y}^\ast$].

What is the relation between those spaces? Let us give a figure of the mappings.

image::images/FourSpaces.png[alt="Figure of the four involved Banach spaces and mappings between them",{basebackend@docbook:scaledwidth="60%":width=600}]

The matrix latexmath:[$A$] and its dual mapping latexmath:[$A^\ast$], which in
finite dimensions is simply the complex conjugate, are obvious. However, what
are latexmath:[$J_p$] and so forth?

The residual latexmath:[$Ax-y$] gives us the natural direction of error as the
only information we have about the true solution latexmath:[$x$] is given in the
mapped form as the right-hand side latexmath:[$y$]. Therefore, we need a mapping
that preserves angles and lengths with respect to the norm of the respective
space. This role is fulfilled by the so-called duality mappings

latexmath:[$J_p = { x^\ast \in \mathrm{X}^\ast | <x^\ast, x> = ||x||^p_{\mathrm{X}}, ||x^\ast|| = ||x||^{p-1} }$],

where p is associated with the gauge function latexmath:[$t \rightarrow t^{p-1}$].footnote:[In lp-spaces these two p coincide.]

These are connected to the (sub-)gradient of the norm through the famous theorem
of Asplund, latexmath:[$J_p(x) = \frac{\partial}{\partial x} ( \frac 1 p ||x||^p)$]

A sub-gradient is a generalization of the gradient where the space is no longer
smooth and therefore the gradient is no longer unique.

As last but one ingredient we need a new distance measure, replacing the metric
distance that has failed so far in allowing a proof of convergence. We use the
Bregman distance,

latexmath:[$\Delta(x,y) = \tfrac 1 {p^\ast} ||x||^p - < J_p (x), y > + \frac 1 p || y ||^p$]

Finally, we seek for iterative solutions for the above minimization functional,
i.e., we have a procedure latexmath:[$x_{n+1} = x_n + f(x_n, \Delta F(x_n))$],
where the current iterature latexmath:[$x_n$] and the gradient of the functional
to be minimized latexmath:[$F(x_n)$], see above, are used to obtain the next
iterate latexmath:[$x_{n+1}$].

In case of noisy right-hand sides, when no exact solution exists, the iteration
with respect to the minimization functional shows a typical behavior (assuming
we start with the origin as the first iterate): The
data fidelity term decreases and the minimum-norm term increases. There is a
cross-over point where the sum of the two no longer decrease but increases
because the minimum-norm term dominates. This gives rise to a so-called *"L"-
shape* and requires a stopping rule for the minimization as the optimal solution
lies at the kink of the L. This is called *semi-convergence*.

All of this can be found in the following nice-to-read academical papers:

- link:#Schoepfer2006[[Schoepfer2006]]
+
First paper on a non-linear Landweber method using Duality Mappings for general
smooth, convex Banach spaces. Proof of strong convergence in noise-free case.
+
- link:#Schoepfer2008[[Schoepfer2008]]
+
Paper on the underlying relation between Metric and Bregman projections that
form the essential basis for the Sequential Subspace Methods. Multiple search
directions. Proof of weak convergence.
+
- link:#Schoepfer2009[[Schoepfer2009]]
+
Extension to perturbed right-hand sides. Proof of strong convergence for
specific set of multiple search directions. Fast method when using two search
directions by projection onto hyperplanes.
+
- link:#Heber2019[[Heber2019]] (also available as arXiv preprint link:#Heber2016[[Heber2016]]).
+
Acceleration of the Sequential Subspace Methods by using metric projections to orthogonalize search directions. Connection to Conjugate Gradient Minimal Error
method.

If you want to constrain yourself to a single paper, then we recommend the last
one as it formed the basis for this library.

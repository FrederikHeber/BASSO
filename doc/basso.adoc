BAnach Sequential Subspace Optimizer
====================================
// Web page meta data
:Author:        Frederik Heber
:Email:         frederik.heber@gmail.com
:keywords:      BASSO, Banach space, minimization, linear system of equations
:description:   BASSO is a library written in cpp for solving underdetermined or overdetermined linear systems of equations Ax=y.
:Date:          {revdate}
:Revision:      {revnumber}
:toc:
:toc-placement: preamble
:doctype:       book
:numbered:
:imagesdir:     {imagedir}
:language:      {basebackend@docbook:C++:cpp}

.{revdate}: BASSO {revnumber}
******************************************************************************
BASSO is a C++-library solving overdetermined or underdetermined linear systems of equations in general Banach spaces. Its interface mimicks the same concepts in terms of classes and functions that are also needed for the mathematical proof of convergence, namely spaces, norms, vectors, and (duality) mappings. In other words, vectors are uniquely associated to a specific space instance and calculating a scalar product of vectors from different spaces will fail. This allowed a very robust method of implementing the sequential subspace optimization methods that can be considered a generalization of the the Conjugate Gradient Minimal Error method from Hilbert spaces to Banach spaces. Overall, this provides a very efficient library to solve linear systems of equations with general l_p regularization. Moreover, it also allows to calculate low-rank matrix factorizations.

BASSO received funding from the German Ministry for Research and Education (BMBF) through the project HYPERMATH.

'Frederik Heber'
******************************************************************************

Introduction
------------

One of the most ubiquitous tasks in computing is solving linear systems of equations, where a matrix latexmath:[A] and a right-hand side vector latexmath:[y] are given and we are looking for the solution vector latexmath:[x].
Because of the omnipresence of this problem, many different solution methods exist, whose applicability depends on the properties of the matrix: E.g., when the matrix is symmetric, one might use Cholesky factorization. If the matrix is symmetric and positive definite and especially if it is very large, Conjugate Gradient schemes are typically employed.

If the matrix is however more generic, e.g., it is not symmetric, then more general schemes are necessary. Then, we do not have a single space, latexmath:[{\cal X}], but two spaces, latexmath:[{\cal X}] and latexmath:[{\cal Y}]. The solution lives in the former space, the right-hand side in the latter space and the matrix maps in between.

The system of equation is then typically underdetermined or overdetermined and the solution latexmath:[x] is no longer unique. Therefore, an additional criterion is needed that picks the most sensible solution among the admissible ones. This is called regularization.

Typically, (Tikhonov-)regularization is formulated as a minimization problem by adding a penalty term as follows:

[latexmath]
+++++++++++
min_x ( || Ax-y ||^2 + \lambda ||x||^2)
+++++++++++

This minimization problems contains two terms: the first is called the data fidelity term that minimizes the residual, i.e. the metric distance to the sets of admissible solutions. The second term is the regularization term that enforces a minimum-norm solution.

Depending on the employed norms in this minimization problem the solutions no longer live in a Hilbert space but in a more general Banach space, e.g., the N-dimensional vector space latexmath:[\mathrm{R}^N] equipped with the norm latexmath:[\ell_{1.1}] norm.  Convergence theoretical results are well-established for Hilbert spaces. For Banach spaces the situation is more difficult:
In a Banach space the space and its dual do not in general coincide, namely they are not in general isometric isomorphic to each other.

This in turn causes us to deal not with two spaces, latexmath:[{\cal X}] and latexmath:[{\cal Y}], but with possibly up to four spaces, latexmath:[{\cal X}], latexmath:[{\cal X}^\ast], latexmath:[{\cal Y}], and latexmath:[{\cal Y}^\ast].

What is the relation between those spaces? Let us give a figure of the mappings.

image::images/FourSpaces.png[Figure of the four involved Banach spaces and mappings between them]

The matrix latexmath:[A] and its dual mapping latexmath:[A^\ast], which in finite dimensions is simply the complex conjugate, are obvious. However, what are latexmath:[J_p] and so forth?

The residual latexmath:[Ax-y] gives us the natural direction of error as the only information we have about the true solution latexmath:[x] is given in the mapped form as the right-hand side latexmath:[y]. Therefore, we need a mapping that preserves angles and lengths with respect to the norm of the respective space. This role is fulfilled by the so-called duality mappings

[latexmath]
+++++++++++
J_p = { x^\ast \in {\cal X}^\ast | <x^\ast, x> = ||x||^p_{\cal X}, ||x^\ast|| = ||x||^{p-1} },
+++++++++++

where p is associated with the gauge function latexmath:[t \rightarrow t^{p-1}].footnote:[In lp-spaces these two p coincide.]

These are connected to the (sub-)gradient of the norm through the famous theorem of Asplund, latexmath:[J_p(x) = \frac{\partial}{\partial x} ( \tfrac 1 p ||x||^p)]

A sub-gradient is a generalization of the gradient where the space is no longer smooth and therefore the gradient is no longer unique.

As one last ingredient we need a new distance measure, replacing the metric distance that has failed so far in allowing a proof of convergence. We use the Bregman distance,

[latexmath]
+++++++++++
\Delta(x,y) = \tfrac 1 {p^\ast} ||x||^p - < J_p (x), y > + \tfrac 1 p || y ||^p
+++++++++++

All of this can be found in the following nice-to-read academical papers:

- link:#Schoepfer2006[[Schoepfer2006]]
+
First paper on a non-linear Landweber method using Duality Mappings for general smooth, convex Banach spaces. Proof of strong convergence in noise-free case.
- link:#Schoepfer2008[[Schoepfer2008]]
+
Paper on the underlying relation between Metric and Bregman projections that form the essential basis for the Sequential Subspace Methods. Multiple search directions. Proof of weak convergence.
- link:#Schoepfer2009[[Schoepfer2009]]
+
Extension to perturbed right-hand sides. Proof of strong convergence for specific set of multiple search directions. Fast method when using two search directions by projection onto hyperplanes.
- link:#Heber2019[[Heber2019]] (also available as arXiv preprint link:#Heber2016[[Heber2016]]).
+
Acceleration of the Sequential Subspace Methods by using metric projections to orthogonalize search directions. Connection to Conjugate Gradient Minimal Error method.

If you want to constrain yourself to a single paper, then we recommend the last one as it formed the basis for this library.

Overview
--------

This mathematical introduction is necessary to understand the elements contained in this library and put them to the best possible use.

=== The Basic Elements

BASSO mimicks all of these objects we have introduced before in its class structure: There are vector space classes `NormedSpace` and `NormedDualSpace`. There are norm class `L1Norm`, `LpNorm`, and `LInfinityNorm`. There is a vector class `SpaceElement`. There are mapping classes `DualityMapping` -- with specializations `LpDualityMapping`, `L1DualityMapping`, `LInfinityDualityMapping` -- `LinearMapping` (for matrices), and `TwoFactorLinearMapping` (for product of two matrices used in factorization problems). Finally, there are function classes/functors such as `BregmanDistance`, `ResidualFunctional`, `SmoothnessFunctional`.

[NOTE]
====
BASSO is based on the fantastic link:http://eigen.tuxfamily.org/[Eigen] library for all linear algebra-relates routines. Therefore it uses the Eigen classes for representing vectors and matrices.
====

=== Inverse problem and Solver classes

These can be considered the basic elements. Moreover, it contains some more complex classes that comprise the linear system of equations to be solved or the overall problem structure, `InverseProblem`.

In order to solve this inverse problem of latexmath:[Ax=y] we need solvers. Their classes are called `LandweberMinimizer`, `SequentialSubspaceMinimizer` or `SequentialSubspaceMinimizerNoise` (with additional noise in the right-hand side taken into account).

=== General usage concepts

Specific instances of these classes, e.g., the `NormedSpace` class, are created using the so-called *Factory* pattern by classes that typically share the same name just with an added Factory, e.g., `NormedSpaceFactory`. These contain a function `create()` that is called with certain parameters and returns a pointer to the created instance.

NOTE:[All these pointers are wrapped into `boost::shared_ptr` to prevent memory loss, i.e. they deallocate automatically when reference to the pointer is held.]

All things associated to an object, e.g., the norm of a space, can be queried from the instance using getters, e.g.,  `NomedSpace.getNorm()` for the norm, `NormedSpace.getDimension()` for the number of degrees of freedom of this space, or `NormedSpace.getDualSpace()` for its dual space. See <<NormedSpace>> for details.

=== Rough sketch of inverse problem solving

This should give you a rough idea of how to solve an inverse problem

1. Create the two Banach spaces as `NormedSpace` by supplying the respective lp norm parameter. Their dual spaces are created automatically and so are in turn the required duality mappings.
2. Create the right-hand side latexmath:[y] as a `SpaceElement` by using `NormedSpace.createElement()` or `ElementCreator.create()` giving the associated space and the link:http://eigen.tuxfamiliy.org[Eigen] vector.
3. Create the linear mapping latexmath:[A] using `LinearMappingFactory.createInstance()` giving the two spaces and the link:http://eigen.tuxfamiliy.org[Eigen] matrix.
4. Create the inverse problem `InverseProblem` giving the mapping, the two spaces and the right-hand side.
5. Create sensible starting values in the solution space and its dual. Note that zero is always a admissible starting value.
6. Finally, instantiate a solver such as `SequentialSubspaceMinimizer` give it the inverse problem, starting value and dual starting value and a true solution if known.

The result will be a structure wherein the approximate solution and a few other instances such as the residual or the number of iterations and the status are contained.

The first four steps can be combined using `InverseProblemFactory` that takes certain string arguments describing the spaces and the right-hand side and the matrix in link:http://eigen.tuxfamily.org[Eigen]-format.

=== Inverse Problems with auxiliary constraints

For more complex problems a *split feasibility* ansatz is used where each constrain is fulfilled in turn in an iterative fashion. To this end, there is the abstract `FeasibilityProblem` class, the `AuxiliaryConstraintsProblem` which combines it with an arbitrary combination of `AuxiliaryConstraints` such as unity, non-negativity or a logical combination of these. Finally, there is the high-level class `SplitFeasibilitySolver` that is solved inverse problems with these additional auxiliary constraints. These constraints are simply `register()` with this class. See <<SplitFeasibilitySolver>> for details.

Let us alook at an example where we solve the random matrix toy problem. We create aa random matrix and a random right-hand side. We make sure that the right-hand side is actually in the range of the matrix. We construct all instances necessary for the solver and finally solve the inverse problem.

[NOTE]
====
We hide away all includes and two static functions for creating a random vector and a random matrix in the include files *random_matrix_inverseproblem_?.hpp*
====

.Inverse Problem: Random matrix, longer version
[source,{basebackend@docbook:C++:cpp}]
----
include::listings/random_matrix_inverseproblem_1.cpp[]
----

As you probably notice, BASSO is at the moment focused on providing small executables that solve specific (inverse) problems: We have set the options in quite a crude fashion. It is much more elegant to query them from the user and parse them from `(argc, argv)`. Above we have tried to rely on them as little as possible.

Let us look again at the above example. This time we create a `CommandLineOptions` at the start and use all the convenience objects there are.

.Inverse Problem: Random matrix, short version
[source,{basebackend@docbook:C++:cpp}]
----
include::listings/random_matrix_inverseproblem_2.cpp[]
----

In the following we look at some example programs that have been implemented using the library. We will only discuss the general problem and how the executable is called. Much of it is actually along the same lines as before.

Examples
--------

We have implemented a number of typical inverse problems using BASSO. These are command-line programs that take all required parameters as arguments using the  `boost::program_options` library. See the `Examples` folder and all subfolders therein.

Note that basically all examples come with an additional program `...Configurator` that writes command-line options into a file. This is to allow reproducible runs where all employed options can be inspected at any time for a given result. At any later point in time you can easily see what options produced the respective output files by looking at this file.

=== BASSO

The main program which solves any kind of inverse problem given in the form of two files containing the matrix and the right-hand sie vector has the same name as the library itself, namely *BASSO*.

In the following we use it to solve a simple problem which has been investigated in link:#Schoepfer2006[[Schoepfer2006]]: Determine the point on a line (in two dimensions) which has minimal distance to the origin in the latexmath:[\ell_p]-norm.

[latexmath]
+++++++++++
A = \begin{pmatrix} 2 & 1 \\ 2 & 1 \end{pmatrix}, \quad y = \begin{pmatrix} 2 \\ 2 \end{pmatrix}
+++++++++++

We encode the *matrix* A and the *vector* y in the files 'matrix.m' and 'vector.m', respectively.

NOTE: BASSO uses the _MatLab/Octave file format_ with its typical '.m' suffix in its file formats. This allows to use Octave to easily create, read, and write matrix or vectors from and to files. See link:https://octave.org/doc/v4.2.1/Simple-File-I_002fO.html[Simple File I/O] for details. The file parsing is handled by the `MatrixIO` class, see link:#MatrixIO[].

[source,bash]
-------------
Basso
                --algorithm Landweber
                --type-space-x "lp"
                --px 2
                --powerx 2
                --type-space-y "lp"
                --py 2
                --powery 2
                --delta 0.0001
                --C 1
                --maxiter 50
                --iteration-file Landweber1.db
                --matrix ${CMAKE_CURRENT_SOURCE_DIR}/pre/matrix.m
                --rhs ${CMAKE_CURRENT_SOURCE_DIR}/pre/vector.m
                --solution straightline1.m
-------------

In this call of 'Basso' we use the Landweber with latexmath:[{\cal X}] and latexmath:[{\cal Y}] being latexmath:[\ell_2]-spaces. We terminate when the residual is less than 1e-4 and use at most 50 iterations. All information on the iterations is written to an SQLite database file 'Landweber1.db'. The solution, i.e. the point closest to the origin on the line described by latexmath:[A] is written to 'straightline1.m'.

=== Gravity

In the very nice book link:#Hansen2010[[Hansen2010]] an example of finding the gravitational constant is given. We have implemented this example in the context of BASSO.

[quote, Discrete Inverse Problems - Insight and Algorithms, P.C. Hansen]
_____
An unknown mass distribution with density f(t) is located at depth d below the surface, from 0 to 1 on the t axis [shown in Figure 2.1]. We assume there is not mass outside the source, which produces a gravity field everywhere. At the surface along the s axis (see the figure) from 0 to 1, we measure the vertical component of the gravity field, which we refer to as g(s).
_____

This is the problem we have to solve: There is a one-dimensional manifold with "mass" at some distance d to our measurement device. At the surface -- the one dimensional manifold where our device sits -- we feel the combined effect of the mass on the whole manifold but always diminished by the distance to the respective point mass at f(t). We have information only of the measured gravity field g(s) and now want to deduce the mass distribution f(t). This is our inverse problem: We have the cause. Now we want to know what's the source of it.

What we need is a equation that relates the two: This is known by the name of Fredholm integral equation of the first kind, see link:#Hansen2010[[Hansen2010]].

[latexmath]
+++++++++++
\int_0^1 K(s,t)  f(t) dt = g(s), \quad 0 \leq s \leq 1
+++++++++++

latexmath:[K(s,t)] here is called the "kernel". If the kernel is linear and we discretize the x axis by using a set of equidistant points along it, we obtain a matrix and the equation becomes the typical inverse problems that we know already: latexmath:[Ax=y].

[latexmath]
+++++++++++
K(s,t) = \frac{d} {\bigl (d^2+(s-t)^2 \bigr)^{\tfrac 3 2}}
+++++++++++

This equation for the kernel results from knowing that the magnitude of the gravity field along s behaves as latexmath:[f(t)dt/r^2], where latexmath:[r = \sqrt{d^2 + (s-t)^2}] is the distance between the source point s and the field point t.

[source,bash]
-------------
include::[]
-------------

=== ComputerTomography

In computerized tomography the inverse problem is the reconstruction of the
inside of an object from projections. Measurements are for example obtained by
passing radiation through a body, whereby their intensity is diminished,
proportional to the passed length and density f(x) of the body:
latexmath:[[0,1]^2 \rightarrow \reel^+_0]. This decrease is measured over a
latexmath:[a] angles and latexmath:[s] shifts of radiation source and detectors.
This measurement matrix is usually called *sinogram*. Again, we follow
link:#Hansen2010[[Hansen2010, ยง7.7]].

In (2D) tomography much depends on how the measurement is obtained. Rays might
passe through the body in parallel or in fan-like formation. This depends on
the setup of the detectors. If there is just a single, point-like detector
that is rotated in conjunction with the radiation source. The detector might
also be spread out, detecting along several small line segments.

In our case we assume single rays, these are suitable parametrized. When the
radiation passes through an object, the effect on its intensity is described
by the Lambert-Beer law, latexmath:[dI = I_0 \exp{ f(x) dx}]. Given some
initial intensity latexmath:[I_0] it is diminished when travelling dx through
the body f(x) by dI.

This can be written as the famous *Radon transform*,

[latexmath]
+++++++++++
b_i = \int_{-\infty}^{\infty} f(t^i(\tau)) d\tau,
+++++++++++

where i enumerates all rays passed through the body and latexmath:[t^i] is the
respective parametrization in 2D, latexmath:[t^i = t^{i,0} + \tau d^i] with
the direction of the ray d and latexmath:[i=s \cdot a] rays in total.

The problem is typically discretized in a pixel basis and for setting up the
Radon matrix we need to count the length of the ray passing through each of
the pixels.

NOTE: The pixels in 2D are vectorized and the matrix is over all pixels and
over all grid points of the body's density f(x).

Eventually, we obtain a matrix equation: latexmath:[b_i = \sum_j a_{ij} x_j].

Again, this is the inverse problem that we need to solve.

The example program contained with BASSO is called 'ComputerTomography'.

=== MatrixFactorizer

One last and more complicated program that has been investigated in the course
of the BMBF funded project link:https://www.math.uni-sb.de/hypermath/[HYPERMATH]
is to matrix factorization as two alternating inverse problems.

In HYPERMATH we looked at hyperspectral images. These are images that contain
now only three colors but thousands of different channels. These images were
taken by many different measurement methods: Near-Infrared Spectroscopy (NIR),
Matrix Assisted Laser Deposition and Imaging (MALDI), Raman spectroscopy or
Spark Optical Emission Spectroscopy (OES). The channels were either truly
electromagnetic channels (NIR, Raman), particles masses (MALDI) or elemental
abundances (OES). The images were 2D measurement grids of different objects.

Hyperspectral images are typically very large. 1e6 to 1e8 number of pixels
and 1e3 to 1e5 number of channels are typically encountered, causing up to
1e12 degrees of freedom to be stored. Hence, these are big data.

One central problem is therefore storage. The central idea of the HYPERMATH
project is to use matrix factorization. The measurements are seen as a large
matrix where one dimension is the number of pixels and the other is the number
of channels. The underlying assumption is that the measurement is caused by
the linear superposition of a few *endmembers*. For example, in Raman
spectroscopy biological tissue is investigated. Spectrographical patterns
are caused by the molecules that make up the tissue.

(Non-negative) Matrix factorization will then decouple the superimposed effects
of all these endmembers. It splits the measurement matrix A into two factors
K and X, where K contains relations from channels to endmembers and X from
endmembers to pixels, latexmath:[A = K \cdot X].

If the number of reconstructed endmembers is much smaller than the number of
pixels or channels, then the storage requirement of both K and X is much
smaller than of the original matrix A.

This problem can be written as two different inverse problems: First, consider
each row in X and each row in A. We obtain latexmath:[A_i = K X_i], where K is
the matrix to formally invert. The other way round, looking at rows, we obtain
latexmath:[A_j = K_j X]. Now, X is the matrix to invert.

Next, we perform an alternating least squares minimization, where each inverse
problem is solved in turn for a few iterations. This will converge to the
two factors we are looking for.

Moreover, this problem can be seen as a split feasibility problem, where
each constraint is addressed one by the other. This allows to add further
constrains on the factors K and X. For example, row sums might need to be
unity or similar.

In the example program 'MatrixFactorizer' this method is implemented.

Helper programs
---------------

BASSO comes with several small helper programs that aid in setting up problems,
perturbing right hand sides or projecting onto the range of the given matrix.

=== NoiseAdder

'NoiseAdder' adds a specific amount of white noise to a given vector.

=== RangeProjector

'RangeProjector' projects onto the range of a matrix. This is again an inverse
problem. The program however sets it up as a split feasibility problem to allow
additional constraints to be fulfilled in the projection, e.g. non-negativity.

=== MatrixToPNG

'MatrixToPNG' converts a vector into a PNG image file. Although images are 2D
and therefore one would expect a matrix. In computerized tomography, the image
information is vectorized. Hence, this program expects a vector.

It supports an arbitrary color table, e.g., *blackwhite* or *bluegreenred*.

The image can be automatically flipped or mirrored.

=== RadonMatrixWriter

'RadonMatrixWriter' writes the Radon transform discretized in a pixel basis to
a file such that it forms the matrix in the inverse problem of 2D computerized
tomography.

Reference
---------

[[NormedSpace]]
=== NormedSpace and NormedSpaceFactory

A finite-dimensional (Banach) space is instantiated using the `NormedSpace`
class by giving its constructor the dimension of the space and a norm object,
`NormedSpace X(200, norm);``

However, a norm cannot be constructed without a space. Both are tightly
interlinked in BASSO. Hence, the typical way of setting up a space is by
using the `NormedSpaceFactory`:

[source,{basebackend@docbook:C++:cpp}]
------------
#include "Minimizations/InverseProblems/InverseProblems/Factory.hpp"
#include "Minimizations/Space/NormedSpaceFactory.hpp"

InverseProblemFactory::args_t args_SpaceX;
args_SpaceX += boost::any(1.1), boost::any(2.);
NormedSpace__ptr_t X = NormedSpaceFactory::create(200, "lp", args_SpaceX);
------------

Here, we have need to specify not only the dimension but also the properties
of the norm. We set the type of the norm to be "lp" (or "regularized_l1"), see
the static function `NormFactory::getMap()` for all options.

Moreover, we need to give the spaces parameters. An lp-space needs the value of
p and also the power type to create the associated duality mapping. As the
number of parameters differ over the different available norms, they are given
as a vector of `boost::any` arguments. Here, we set p to 1.1 and the power type
to 2.

Finally, the `NormedSpaceFactory::create()` function returns the constructed
instance of the `NormedSpace` class. Note that it is wrapped on a
`boost:shared_ptr` and is deconstructed automatically when it is no longer used.
In other words, don't worry about it, just use it like a static instance.
The type has kept the `.._ptr_t` ending to remind you that you need to access
its members like `X->getNorm()`, i.e. like a normal pointer.

[[SpaceElement]]
=== SpaceElement

The `SpaceElement` class wraps a `Eigen::VectorXd` and associates it directly
with a certain space. All typical linear algebra routines such as vector
product, scalar product, addition, ... can be found in this class.

Let us give a few example calls that should be self-explanatory.

[source,{basebackend@docbook:C++:cpp}]
------------
#include <iostream>
#include "Elements/SpaceElement.hpp"
#include "Norms/Norm.hpp"

SpaceElement_ptr_t x,y;

// linear algebra
const double scalar_product = (*x) * (*y);
SpaceElement_ptr_t scalar_multiplication = 5. * (*x);
SpaceElement_ptr_t addition = (*x) + (*y);
const double norm = (*x->getSpace()->getNorm())(x);

// access components
(*x)[5] = 5.;
x->setZero();

// comparators
const bool equality = (*x) == (*y);
// don't; x == y (this compares the addresses in memory)

// print element
std::cout <<
------------

The association with the space prevents use in other spaces even if they share
the number of degrees of freedom.

[[Norm]]
=== Norm

A norm basically has the single function to return the "norm" of a given
element. The `Norm` associated with a space makes it a `NormedSpace`.

Note that certain norms cause spaces to be smooth or not. Hence, the norm can
be queried for `isSmooth()`.

`Norm` only defines the general interface. Specializations are `LpNorm`,
`L1Norm`, `LInfinityNorm` that implement the specific norm function to use.

Norms throw an `NormIllegalValue_exception` when they are called with a
parameter outside expected intervals, e.g. an illegal p value such as -1.

[[Mapping]]
=== Mapping

Similar to `Norm`, `Mapping` defines the general interface of a function that
maps an element from one `NormedSpace` onto another `NormedSpace` instance.
Therefore, its constructor requires a source and a target space instance.

[source,{basebackend@docbook:C++:cpp}]
------------
#include "Elements/SpaceElement.hpp"
#include "Mappings/LinearMappingFactory.hpp"

NormedSpace_ptr_t X, Y;
SpaceElement_ptr_t x;
Eigen::MatrixXd matrix; // given from outside

LinearMapping_ptr_ A = LinearMappingFactory:create(X,Y, matrix);

SpaceElement_ptr_t y = (*A)(x);
------------

Its `operator()` then performs the actual mapping, taking a `SpaceElement` from
the source space, performing alterations on the element's components, and
returning a `SpaceElement` in the target space.

A more complicated example is as follows where we have two matrix factory
whose product forms the actual linear mapping.

[source,{basebackend@docbook:C++:cpp}]
------------
#include <cassert>
#include "Elements/SpaceElement.hpp"
#include "Mappings/LinearMappingFactory.hpp"
#include "Spaces/NormedSpace.hpp"

NormedSpace_ptr_t X, Y; // instantiated before
SpaceElement_ptr_t x;   // instantiated before, associated with X
assert ( x->getSpace() == X );
Eigen::MatrixXd factor_one; // given from outside
Eigen::MatrixXd factor_two; // given from outside

LinearMapping_ptr_ A = LinearMappingFactory:createTwoFactorInstance(
    X,Y, factor_one, factor_two);

SpaceElement_ptr_t y = (*A)(x);
------------

WARNING: Take care that the matrix dimensions match. Otherwise, an assertion is thrown.

TIP: A linear mapping can be decomposed into an singular value decomposition, see `LinearMapping::getSVD()` and `SingularValueDecomposition`.

An example of such a mapping is the `LinearMapping`. This is a specialization
that implements a multiplication with a matrix associated with the mapping.

Moreover, `DualityMapping` is another example. There, the constructor only needs
a single space as each space knows about its dual space and this mapping always
maps into the dual space. In the folder 'Mappings/Specifics' specific
implementations of duality mapping, such as the one in lp-spaces, are contained.

Both these classes have specific factory classes, namely `LinearMappingFactory`
and `DualityMappingFactory` to create the respective instance types. These
factories are also used internally, e.g., when constructing the space with its
dual space and the associated duality mappings that all have to be interlinked.
To this end, these accept "weak" `NormedSpace` ptr instances.

Finally, each mapping is inherently associated with an adjoint mapping. For
`LinearMapping` this would be the transposed of a (real-value) matrix.

[[Functions]]
=== Functions

Functions are similar to `Mappings` and to `Norms`. They take one or two
arguments being `SpaceElement` instances and produce a real value.

- `BregmanDistance`
- `ResidualFunctional`
- `SmoothnessFunctional`
- `SmoothnessModulus`
- `VectorProjection`

Let us give an example using the `BregmanDistance`.

[source,{basebackend@docbook:C++:cpp}]
------------
#include "Minimizations/Elements/SpaceElement.hpp"
#include "Minimizations/Functions/BregmanDistance.hpp"
#include "Minimizations/Mappings/DualityMapping.hpp"

Norm_ptr_t norm; // instantiated before, e.g. Space::getNorm()
DualityMapping_ptr_t J_p; // instantiated before, e.g. Space::getDualityMapping()
BregmanDistance delta(norm, J_p, J_p->getPower());

SpaceElement_ptr_t x,y; // instantiated before

const double bregman_distance = delta(x,y);
------------

Some functions are used solely internally, namely for minimization.
- `BregmanProjectionFunctional`
- `MetricProjectionFunctional`
- `VectorProjection_BregmanDistanceToLine`

[[MatrixIO]]
=== MatrixIO

`MatrixIO` is a namespace wherein several helper functions are gathered that
read or write matrix from or to files. Moreover, there are helper functions to
print matrices to output streams.

[source,{basebackend@docbook:C++:cpp}]
------------
#include <Eigen/Dense>
#include "MatrixIO/MatrixIO.hpp"

Eigen::VectorXd solution;

// parse from Octave-style file
MatrixIO::parse("start_solution.m", "rhs", solution);

// write to Octave-style file
MatrixIO::store("solution.m", "rhs", solution);
------------

This will parse a vector from the file 'start_solution.m' into the
`Eigen::VectorXd` instance named `solution`. Here, *rhs* is used in error
messages when parsing, e.., when the file does not exist.
Similary, we write the vector into 'solution.m' from the same instance.

When writing `SpaceElement`s (instead of `Eigen::VectorXd`) we recommend using
the convenience class `SpaceElementWriter`.

[source,{basebackend@docbook:C++:cpp}]
------------
#include <fstream>
#include "Minimizations/Elements/SpaceElementIO.hpp"
SpaceElement_ptr_t solution;

// parse from file
std::ifstream ist("start_solution.m");
SpaceElementWriter::input(ist, solution);

// write to file
std::ofstream ost("solutionm.m");
SpaceElementWriter::output(ost, solution);
------------

[[InverseProblem]]
=== InverseProblem

The `InverseProblem` class takes a mapping, two space instances, and a
right-hand side. It is simply a convenience struct that keeps all the necessary
references at hand, namely it constructs internal shorthand references to all
items required for the minimizations methods such as the dual spaces, the
adjoint mapping. Moreover, it contains the solution element to be found.

[[Minimizers]]
=== Minimizers

Minimizers form the core of BASSO. They solve the minimizations problems that
produce solutions to inverse problems or split feasibility problems.

The general interface is defined in `GeneralMinimizer`. Its constructor takes
all controlling parameters in the form of `CommandLineOptions` and moreover
an `InverseProblem` instance and a `Database` whereto iteration-related
information is stored.
Its `operator()` expects the inverse problem (as you may change the matrix A
and the right), a starting value together with its dual starting value and a
possibly known solution to calculate the Bregman distance to it.

The following minimizers are implemented:

- `LandweberMinimizer`
+
The Landweber method is the traditional method of inverse problems for which
strong convergence was shown first. In practice, it is known to converge very
slowly. This is especially true when a fixed step width is used. However, even
with a line search such as using Brent's method it is not competitive with the
other SESOP Minimizers. One advantage is naturally that it does not need any
derivative information, i.e. it works even when the space X is not smooth such
as in case of the latexmath:[\ell_1]-norm.
- `SequentialSubspaceMinimizer`
+
The Sequential Subspace Method (SESOP) is the main workhorse in BASSO. It can
employ multiple search directions that can even be made (g-)orthogonal in the
context of Banach space to speed up convergence. Note that SESOP is constrained
to uniformly smooth and convex Banach spaces, i.e. latexmath:[p \in (1,\infty)].
Current work is underway to extend to the uniformly convex Banach spaces.
- `SequentialSubspaceMinimizerNoise`
+
This is variant of SESOP with additional regularization. In other words, it may
be used for perturbed right-hand sides. Moreover, this variant is suited also
for non-linear minimization, see link:#Wald2017[[Wald2017]].

[NOTE]
======
Empirical tests have shown that multiple orthogonal search directions in
lp-spaces are only efficient for latexmath:[p>3]. If p is smaller only a single
search direction is recommended as more search directions just bring additional
complexity (because of the additional orthogonalization operations) but do not
increase convergence speed. See link:#Heber2019[[Heber2019]] for details.
======

These Minimizers may be cleanly instantiated using the `MinimizerFactory`.

An important point is how the step width is determined. This is especially
important for the `LandweberMinimizer`. There are several different variants
implemented. All of which can be easily accessed through the
`DetermineStepWidthFactory`.

The SESOP Minimizers determine the step width automatically through a line
search. However, this line search may be tuned to be only inexact by setting
appropriate Wolfe conditions.

[[CommandLineOptions]]
=== CommandLineOptions

Many parameters that control how for example the minimization is performed
are wrapped in a class `CommandLineOptions`.

Let us briefly discuss what the class looks like and what it can do.

First of all, `CommandLineOptions` is basically a `struct` containing all sorts
of parameters that control what Banach spaces (and what norms) are employed,
what kind of Minimizer, what stopping criteria and so on.

Furthermore, the class has helper functions to quickly parse the state from
given command-line options. This explains the nature of its name.

Moreover, the class' state can be stored to a human-readable file using
`boost::serialization`. It is written as in the following example code.

[source,{basebackend@docbook:C++:cpp}]
--------
#include <boost/progam_options.hpp>
#include <fstream>
#include "Options/CommandLineOptions.hpp"

using namespace boost::program_options as po;

CommandLineOptions opts;

// write to file
std::ofstream config_file("config.cfg");
opts.store(config_file);

// read from file
std::ifstream config_file("config.cfg");
po::store(po::parse_config_file(config_file, opts.desc_all), opts.vm);
po::notify(vm);
--------

Finally, `CommandLineOptions` has functions to check the validity of each
parameter.

[[StoppingCriterion]]
=== StoppingCriterion

A `StoppingCriterion` takes several values related to the iteration, namely
the time passed, the number of outer iterations, the current residuum and
the norm of the right-hand side, and decides whether it is time to stop or not.

There are several primities such as `CheckRelativeResiduum` or `CheckWalltime`
that may be arbitrarily combined using boolean logic.

It is simplest to employ the factory for producing such a combined stopping
criterion. It creates the instance from a simple string as follows.

[source,{basebackend@docbook:C++:cpp}]
------------
#include "Minimizations/Minimizers/StoppingCriteria/StoppingArguments.hpp"
#include "Minimizations/Minimizers/StoppingCriteria/StoppingCriteriaFactory.hpp"
#include "Minimizations/Minimizers/StoppingCriteria/StoppingCriterion.hpp"

StoppingArguments args;
args.setMaxIterations(100);

StoppingCriterion::ptr_t criterion = StoppingCriteriaFactory::create(
  "StoppingArguments || StoppingArguments"
  )
------------

See the `StoppingCriteriaFactory::StoppingCriteriaFactory()` for a list of all
known criterion and boolean operators (they are the familar ones from cpp).

[[Solvers]]
=== Solvers

The general interface to all solvers is defined in `FeasibilityProblem`. It is
basically simply a functor, i.e. having a single `operator()` function that
executes the solving.

There are some convenience functions such as `getName()` which returns a human
readable name for ouput purposes. `clear()` and `finish()` are used when doing
split feasibility problems with multiple iterations.

Then we have several specific solvers which we discuss each in turn:

[[InverseProblemSolver]]
==== InverseProblemSolver

The `InverseProblemSolver` class takes an inverse problem and applies a
minimizer method. Additionally, it connects with a `Database` class to write
iteration-specific information to that. This allows to check on the quality of
the iteration process lateron.

The parameters that control the manner of the solving, e.g., the minimizer
method and its options, are given using a `CommandLineOptions` instance.q

[[RangeProjectionSolver]]
==== RangeProjectionSolver

The `RangeProjectionSolver` class directly takes a matrix and a right-hand side
and constructs internal spaces for the projection depending on the given
options in the `CommandLineOptions` class. It is very similar to an
`InverseProblem`.

[[AuxiliaryConstraints]]
==== AuxiliaryConstraints

An `AuxiliaryConstraints` is a functor using a `SpaceElement` that enforces a
certain restriction directly on the element. E.g. `NonnegativeConstraint` will
project the vector onto the positive orthant, i.e. all negative components
become zero.

They are used in the context of `SplitFeasibilitySolvers`.

These constraints can be combined using boolean *AND*. Let us give an example
where we use the `AuxiliaryConstraintsFactory`

[source,{basebackend@docbook:C++:cpp}]
------------
#include "Solvers/AuxiliaryConstraints/AuxiliaryConstraintsFactory.hpp"

// create a single non-negative criterion
AuxiliaryConstraints::ptr_t nonneg_criterion =
  AuxiliaryConstraintsFactory::createCriterion(
    AuxiliaryConstraintsFactory::Nonnegative);
AuxiliaryConstraints::ptr_t unit_criterion =
  AuxiliaryConstraintsFactory::createCriterion(
    AuxiliaryConstraintsFactory::Unity);

// create a combination of non-negative and unit
AuxiliaryConstraints::ptr_t combined_criterion =
AuxiliaryConstraintsFactory::createCombination(
      AuxiliaryConstraintsFactory::Combination_AND,
      nonneg_criterion, unit_criterion);
------------

TIP: This can be done directly from a string "Nonnegative && Unity" using `AuxiliaryConstraintsFactory::create()`.

[[SplitFeasibilitySolver]]
==== SplitFeasibilitySolver

A `SplitFeasibilitySolver` is a queue for a number of `FeasibilityProblem` or
`AuxiliaryConstraints`.
They are registered with this queue by using `registerFeasibilityProblem()` and
`registerAuxiliaryConstraints()`, respectively.

The problems are each solved approximately in turn. The queue is repeated over
for as many *loops* as desired or until a given `StoppingCriterion` returns
true.

These split feasibility problems are employed in the context of matrix
factorization but can also be used in inverse problems when additional
constraints apart from the minimum-norm solution requirement need to be
enforced.

To Do
-----

- `GeneralMinimizer`'s takes the `InverseProblem` only out of convenience but it
is hard to understand why it needs it again in `operator()`. Cstor might get
away with just getting a space?
- `FeasibilityProblem::clear()` and `::finish()` can possibly be made protected
as user access is not needed.
- Remove the extra two space arguments in the cstor of `InverseProblem`. They
can be deduced from the Mapping.
- Split `Parameters` of `CommandLineOptions` to disassociate from
command-line context. The former only contains Serialization. Note that there
is already an `Options` class that does the storing.
- `Options` should have a convenience function `parse()` similar to `store()`, see
`Options::parse()` in "config" branch.

:numbered!:

Acknowledgements
----------------

Thanks to all the users of the library!

include::bibliography.adoc[]

/*
 * libbasso.dox
 *
 *  Created on: Jul 15, 2015
 *      Author: heber
 */

/**
 * \page libbasso libBASSO
 *
 * LibBASSO is the implementation of two algorithms for solving inverse
 * problems \f$ Ax = y \f$ in general Banach Spaces, namely the generalized
 * Landweber method and the Sequential Subspace Optimization (SESOP) method.
 *
 * \section general-setup General Setup of the Library
 *
 * There are several high-level structures that need to be employed for
 * specifying an inverse problem and solving it:
 * -# InverseProblem: Contains all spaces, matrices, and vectors that define
 *    the above inverse problem.
 * -# InverseProblemFactory: Constructs the InverseProblem structure from
 *    some other element that each individually specify the spaces involved,
 *    each matrix and each vector.
 * -# SolverFactory: Separate from the InverseProblem is the definition on
 *    how to solve it. For this a Minimizer and some helper objects are needed,
 *    e.g. a Database to write informative values about the minimization to.
 *
 * \section libbasso-landweber Generalized Landweber
 *
 * The Generalized Landweber uses an update of the form
 * \f$ J_p(x^{n+1}) = J_p(x^{n}) + \mu_n A^\ast j_r (A x_n - y) \f$
 * where \f$ J_p \f$ is the duality mapping associated with the space
 * \f$ X \f$ of power type p and \f$ j_r \f$ the single-valued selection
 * of a duality mapping of the space \f$ Y \f$.
 *
 * The solution is obtained via \f$ x_n = J_p^{-1} (J_p(x_n)) \f$, where
 * \f$ J_p^{-1} \f$ is the inverse of the duality mapping \f$ J_p \f$, e.g.
 * in lp spaces it would be the mapping \f$ J_q \f$ with
 * \f$ \frac 1 p + \frac 1 q = 1 \f$.
 *
 * The proof of convergence relies on using the Bregman distance instead
 * of the metric distance.
 *
 * The Landweber method is implemented in the class LandweberMinimizer.
 *
 * \section libbasso-sesop Sequential Subspace Optimization (SESOP)
 *
 * The main idea behind SESOP is to use Bregman projections onto hyperplanes
 * defined by the Landweber search directions \f$ A^\ast j_r (A x_n - y) \f$.
 *
 * These projections can be calculated via the minimization of a simple
 * convex functional whose derivative is known analytically.
 *
 * The advantage is that multiple search directions can be used simultaneously
 * and thus a fast convergence is obtained. If search directions from previous
 * iteration steps are used, expensive matrix-vector calculations are reduced.
 *
 * The SESOP method is implemented in the classes SequentialSubspaceMinimizer
 * and SequentialSubspaceMinimizerNoise for the regularized variant.
 *
 * \section libbasso-sesop-noisy Regularized Sequential Subspace Optimization (RESESOP)
 *
 * A regularized version of SESOP is also implemented. It can so far only
 * be used with up to two search directions.
 *
 * \section libbasso-sesop-orthogonal Orthogonal Sequential Subspace Optimization (OrthoSESOP)
 *
 * SESOP can also use metric projections to semi-orthogonalize search
 * directions among one another. In l2 spaces this then reduces to the well-known
 * Conjugate Gradient method which OrthoSESOP generalizes.
 *
 * The additional orthogonalization causes little overhead but sees strong
 * reduce in iteration steps for a wide range of lp spaces.
 *
 * \section libbasso-warning-norms A word about the norms
 * 
 * Various norms can be chosen such as lp norms or regularized_l1 norms.
 * If an "lp" norm is chosen, then it is advised to have a p value in range
 * range of [1.01, 100] and not outside of that. Note that this p value occurs
 * in the exponent in duality mappings and if very large may easily lead to
 * "inf" when the resulting value exceeds the numerical range. If such small
 * norms are required, then it is advised to resort to "regularized_l1".
 * 
 * \date 2016-04-2021
 */
